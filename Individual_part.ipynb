{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLlib with Atari Learning Environment\n",
    "#### Dimitrios Megkos - dimitrios.megkos@city.ac.uk\n",
    "#### Environment: Atari Freeway (Ram Version) https://gym.openai.com/envs/Freeway-ram-v0/\n",
    "*\"Why did the chicken cross the road?\"*\n",
    "\n",
    "A classic atari game where one player (the agent) controls a chicken who can be made to run across a ten lane highway filled with traffic in order to get to the other side. Every time the chicken successufly crosses the road, a point (1) is awarded to the agent. If the chicken is hit by a car it is pushed back to the bottom of the screen. The point of the game is to collect as many points as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "# Ray and RLlib import\n",
    "import ray\n",
    "# import tune for hyper parameter experimentation\n",
    "from ray import tune\n",
    "from ray.tune import grid_search\n",
    "# Import a Trainable (one of RLlib's built-in algorithms):\n",
    "import ray.rllib.agents.dqn as dqn # Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up and trying multiple configurations\n",
    "There are two ways to experiment with different hyperparameres. \n",
    "The first way is to create different configurations and pass to the trainer inside a for loop that handles the training and stores the rewards which can then be plotted for comparisons.\n",
    "The second and more efficient way is to use Ray's Tune library for experiment execution and hyperparameter tuning. With this method, a grid search can be used to try different hyperparameter combinations, implementing a stoping criteria to control the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Passing different configurations to the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define configurations\n",
    "Two different configurations are defined below, each with different hyperparameters. The first one is a Vanilla DQN, the second is a Double DQN. Epsilon greedy is used to handle the agent's exploration-exploitation rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on lab 07 material\n",
    "# === FIRST CONFIGURATION ===\n",
    "config_1 = dqn.DEFAULT_CONFIG.copy() # copy the default configuration\n",
    "# === Deep Learning Framework Settings ===\n",
    "config_1[\"framework\"] = \"torch\" # set pytorch for the framework\n",
    "# === Environment Settings ===\n",
    "config_1[\"env\"] = \"Freeway-ramDeterministic-v4\" # load Atari Freeway from OpenAI Gym\n",
    "# === Settings for the Trainer process ===\n",
    "config_1[\"gamma\"] = 0.99 # MDP discount factor\n",
    "config_1[\"lr\"] = 0.0001 # the learning rate\n",
    "config_1[\"double_q\"] = False # whether to use double dqn\n",
    "# neural network architecture\n",
    "config_1[\"model\"] = {\n",
    "    \"fcnet_hiddens\": [256,256], # hidden layers and neurons\n",
    "    \"fcnet_activation\": \"relu\" # activation function\n",
    "}\n",
    "# === Exploration Settings ===\n",
    "config_1[\"explore\"] = True\n",
    "config_1[\"exploration_config\"] = {\n",
    "    \"type\": \"EpsilonGreedy\",\n",
    "    # Parameters for the Exploration class' constructor:\n",
    "    \"initial_epsilon\": 1.0,\n",
    "    \"final_epsilon\": 0.01,\n",
    "    \"epsilon_timesteps\": 20000\n",
    "}\n",
    "# === Evaluation Settings ===\n",
    "config_1[\"evaluation_duration\"] = 5\n",
    "config_1[\"evaluation_duration_unit\"] = \"episodes\"\n",
    "config_1[\"evaluation_num_workers\"] = 1\n",
    "config_1[\"evaluation_config\"] = {\n",
    "    \"render_env\": True,\n",
    "    \"explore\": False,\n",
    "}\n",
    "# === Resource Settings ===\n",
    "config_1[\"num_gpus\"] = 0\n",
    "config_1[\"num_cpus_per_worker\"] = 1\n",
    "\n",
    "\n",
    "\n",
    "# === SECOND CONFIGURATION ===\n",
    "config_2 = dqn.DEFAULT_CONFIG.copy() # copy the default configuration\n",
    "# === Deep Learning Framework Settings ===\n",
    "config_2[\"framework\"] = \"torch\" # set pytorch for the framework\n",
    "# === Environment Settings ===\n",
    "config_2[\"env\"] = \"Freeway-ramDeterministic-v4\" # load Atari Freeway from OpenAI Gym\n",
    "# === Settings for the Trainer process ===\n",
    "config_2[\"gamma\"] = 0.99 # MDP discount factor\n",
    "config_2[\"lr\"] = 0.0001 # the learning rate\n",
    "config_2[\"double_q\"] = True # whether to use double dqn\n",
    "# neural network architecture\n",
    "config_2[\"model\"] = {\n",
    "    \"fcnet_hiddens\": [256,256], # hidden layers and neurons\n",
    "    \"fcnet_activation\": \"relu\" # activation function\n",
    "}\n",
    "# === Exploration Settings ===\n",
    "config_2[\"explore\"] = True\n",
    "config_2[\"exploration_config\"] = {\n",
    "    \"type\": \"EpsilonGreedy\",\n",
    "    # Parameters for the Exploration class' constructor:\n",
    "    \"initial_epsilon\": 1.0,\n",
    "    \"final_epsilon\": 0.01,\n",
    "    \"epsilon_timesteps\": 20000\n",
    "}\n",
    "# === Evaluation Settings ===\n",
    "config_2[\"evaluation_duration\"] = 5\n",
    "config_2[\"evaluation_duration_unit\"] = \"episodes\"\n",
    "config_2[\"evaluation_num_workers\"] = 1\n",
    "config_2[\"evaluation_config\"] = {\n",
    "    \"render_env\": True,\n",
    "    \"explore\": False,\n",
    "}\n",
    "# === Resource Settings ===\n",
    "config_2[\"num_gpus\"] = 0\n",
    "config_2[\"num_cpus_per_worker\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with each configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration 1 - Train Agent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 17:57:23,529\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2022-04-13 17:57:26,024\tWARNING trainer.py:2347 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-04-13 17:57:26,026\tINFO simple_q.py:154 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "2022-04-13 17:57:26,026\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-04-13 17:57:26,194\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-04-13 17:57:26,195\tWARNING trainer.py:2347 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-04-13 17:57:26,242\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.1\n",
      "0.1\n",
      "0.8181818181818182\n",
      "0.8181818181818182\n",
      "1.5\n",
      "1.5\n",
      "2.1538461538461537\n",
      "2.1538461538461537\n",
      "2.5\n",
      "2.5\n",
      "3.0\n",
      "3.0\n",
      "3.625\n",
      "3.625\n",
      "4.117647058823529\n",
      "4.117647058823529\n",
      "4.666666666666667\n",
      "4.666666666666667\n",
      "5.052631578947368\n",
      "5.052631578947368\n",
      "5.5\n",
      "5.5\n",
      "5.5\n",
      "5.904761904761905\n",
      "5.904761904761905\n",
      "6.2727272727272725\n",
      "6.2727272727272725\n",
      "6.695652173913044\n",
      "6.695652173913044\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "# based on lab 07 material\n",
    "# initialise ray\n",
    "ray.init()\n",
    "\n",
    "# create our RLlib Trainer.\n",
    "agent_1 = dqn.DQNTrainer(config=config_1)\n",
    "\n",
    "# create list to store rewards\n",
    "avg_rewards_1 = []\n",
    "\n",
    "# begin training\n",
    "for i in range(50):\n",
    "    # Perform one iteration of training the policy with DQN\n",
    "    result = agent_1.train()\n",
    "    #print(pretty_print(result))\n",
    "    print(result['episode_reward_mean'])\n",
    "    avg_rewards_1.append(result['episode_reward_mean'])\n",
    "\n",
    "# shutdown ray\n",
    "#ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Agent 1 Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the rewards of agent 1\n",
    "plt.title(\"Agent 1 - Vanilla DQN\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Rewards\")\n",
    "a1=plt.plot(avg_rewards_1, 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Agent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': 21.0,\n",
       "  'episode_reward_min': 21.0,\n",
       "  'episode_reward_mean': 21.0,\n",
       "  'episode_len_mean': 2048.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 5,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [21.0, 21.0, 21.0, 21.0, 21.0],\n",
       "   'episode_lengths': [2048, 2048, 2048, 2048, 2048]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.08932406426686676,\n",
       "   'mean_inference_ms': 0.5525182832580632,\n",
       "   'mean_action_processing_ms': 0.02557385562628161,\n",
       "   'mean_env_wait_ms': 0.6681889158803357,\n",
       "   'mean_env_render_ms': 1.1005202560566352},\n",
       "  'off_policy_estimator': {},\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate agent for 5 episodes\n",
    "agent_1.evaluate()\n",
    "# shutdown ray\n",
    "ray.shutdown()\n",
    "#show agent_1 evaluation metrics\n",
    "agent_1.evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration 2 - Train Agent 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 18:01:28,354\tWARNING trainer.py:2347 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-04-13 18:01:28,502\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-04-13 18:01:28,502\tWARNING trainer.py:2347 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-04-13 18:01:28,543\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.25\n",
      "0.25\n",
      "0.8888888888888888\n",
      "0.8888888888888888\n",
      "1.8\n",
      "1.8\n",
      "2.5454545454545454\n",
      "2.5454545454545454\n",
      "3.1666666666666665\n",
      "3.1666666666666665\n",
      "4.153846153846154\n",
      "4.153846153846154\n",
      "4.785714285714286\n",
      "4.785714285714286\n",
      "5.533333333333333\n",
      "5.533333333333333\n",
      "6.25\n",
      "6.25\n",
      "6.9411764705882355\n",
      "6.9411764705882355\n",
      "7.555555555555555\n",
      "7.555555555555555\n",
      "8.105263157894736\n",
      "8.105263157894736\n",
      "8.7\n",
      "8.7\n",
      "8.7\n",
      "9.047619047619047\n",
      "9.047619047619047\n",
      "9.545454545454545\n",
      "9.545454545454545\n",
      "10.0\n",
      "10.0\n",
      "10.416666666666666\n"
     ]
    }
   ],
   "source": [
    "# based on lab 07 material\n",
    "# initialise ray\n",
    "ray.init()\n",
    "\n",
    "# create our RLlib Trainer.\n",
    "agent_2 = dqn.DQNTrainer(config=config_2)\n",
    "\n",
    "# create list to store rewards\n",
    "avg_rewards_2 = []\n",
    "\n",
    "# begin training\n",
    "for i in range(50):\n",
    "    # Perform one iteration of training the policy with DQN\n",
    "    result = agent_2.train()\n",
    "    #print(pretty_print(result))\n",
    "    print(result['episode_reward_mean'])\n",
    "    avg_rewards_2.append(result['episode_reward_mean'])\n",
    "\n",
    "# shutdown ray\n",
    "#ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Agent 2 Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the rewards of agent 2\n",
    "plt.title(\"Agent 2 - Double DQN\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Rewards\")\n",
    "a2 = plt.plot(avg_rewards_2, 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Agent 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': 22.0,\n",
       "  'episode_reward_min': 22.0,\n",
       "  'episode_reward_mean': 22.0,\n",
       "  'episode_len_mean': 2048.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 5,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [22.0, 22.0, 22.0, 22.0, 22.0],\n",
       "   'episode_lengths': [2048, 2048, 2048, 2048, 2048]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.0861592968394195,\n",
       "   'mean_inference_ms': 0.54730375860017,\n",
       "   'mean_action_processing_ms': 0.025069873065946623,\n",
       "   'mean_env_wait_ms': 0.6753676874238804,\n",
       "   'mean_env_render_ms': 1.1064958297674363},\n",
       "  'off_policy_estimator': {},\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate agent for 5 episodes\n",
    "agent_2.evaluate()\n",
    "# shutdown ray\n",
    "ray.shutdown()\n",
    "#show agent_2 evaluation metrics\n",
    "agent_2.evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of results\n",
    "- During the first trials, a network architecture of one hidden layer with 512 hidden neurons was used for both Vanilla DQN and Double DQN. Vanilla DQN did not manage to receive any rewards after 50 iterations, achieving a mean reward of zero during evaluation. Double DQN managed to receive 1.20 average rewards after 50 iterations, achieving a mean reward of 11 during evaluation\n",
    "- Changing the architecture of the networks to two hidden layers with 256 hidden neurons each greatly improved the performance of both Vanilla DQN and Double DQN. Having two hidden layers, the agents were able to perform more complex calculations enabling them to learn faster and better. Vanilla DQN started earning rewards after 19 iterations, reaching an average of 7 rewards after 50 iterations and achieving a mean reward of 21 during evaluation. Double DQN started earning rewards after 15 iterations, reaching an average of 10.40 rewards after 50 iterations and achieving a mean reward of 22 during evaluation.\n",
    "- Although both agents achieved similar evaluation scores, Double DQN is the winner here because it not only because it started learning faster, but also because it managed to achieve higher score after 50 training iterations. This makes sense considering a Double DQN is using a second network to calculate the target values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using the Tune Library\n",
    "Using Agent 2 configuration (Double DQN), a grid search is going to be performed in order to experiment more and fine tune the hyperparameters. The parameters that seem to affect learning the most are the *learning rate*, the *number of hidden layers*, the *number of hidden neurons* and the *type of activation function*, therefore the grid search will focus on those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the grid search configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GRID SEARCH CONFIGURATION ===\n",
    "gs_config = dqn.DEFAULT_CONFIG.copy() # copy the default configuration\n",
    "# === Deep Learning Framework Settings ===\n",
    "gs_config[\"framework\"] = \"torch\" # set pytorch for the framework\n",
    "# === Environment Settings ===\n",
    "gs_config[\"env\"] = \"Freeway-ramDeterministic-v4\" # load Atari Freeway from OpenAI Gym\n",
    "# === Settings for the Trainer process ===\n",
    "gs_config[\"gamma\"] = 0.99 # MDP discount factor\n",
    "gs_config[\"lr\"] = grid_search([0.001,0.0001]) # the learning rate\n",
    "gs_config[\"double_q\"] = True # whether to use double dqn\n",
    "# neural network architecture\n",
    "gs_config[\"model\"] = {\n",
    "    \"fcnet_hiddens\": grid_search([[256,256], [256,256,256]]), # hidden layers and neurons\n",
    "    \"fcnet_activation\": grid_search([\"relu\",\"linear\"]) # activation function\n",
    "}\n",
    "# === Exploration Settings ===\n",
    "gs_config[\"explore\"] = True\n",
    "gs_config[\"exploration_config\"] = {\n",
    "    \"type\": \"EpsilonGreedy\",\n",
    "    # Parameters for the Exploration class' constructor:\n",
    "    \"initial_epsilon\": 1.0,\n",
    "    \"final_epsilon\": 0.01,\n",
    "    \"epsilon_timesteps\": 10000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Tuning\n",
    "The tuning will run until the agents reach a mean episode reward of 12 and Tune will return the best hyperparameter combination that required the least total timesteps to reach the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google\n",
    "# initialize ray\n",
    "ray.init()\n",
    "\n",
    "# begin tune analysis\n",
    "gs_analysis = tune.run(\n",
    "    dqn.DQNTrainer, # the trainer\n",
    "    config=gs_config, # use the grid search configuration\n",
    "    stop={\"episode_reward_mean\": 12}, # stop criteria\n",
    "    metric=\"timesteps_total\", # comparisons based on total timesteps\n",
    "    mode=\"min\" # choose based on the least total timesteps\n",
    ")\n",
    "\n",
    "# shutdown ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found: {'num_workers': 0, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 4, 'batch_mode': 'truncate_episodes', 'gamma': 0.99, 'lr': 0.001, 'train_batch_size': 32, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'relu'}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'Freeway-ramDeterministic-v4', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'EpsilonGreedy', 'initial_epsilon': 1.0, 'final_epsilon': 0.01, 'epsilon_timesteps': 10000}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'explore': False}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': 1, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': None, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 1000, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'target_network_update_freq': 500, 'buffer_size': -1, 'replay_buffer_config': {'type': 'MultiAgentReplayBuffer', 'capacity': 50000}, 'store_buffer_in_checkpoints': False, 'replay_sequence_length': 1, 'lr_schedule': None, 'adam_epsilon': 1e-08, 'grad_clip': 40, 'learning_starts': 1000, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': True, 'hiddens': [256], 'double_q': True, 'n_step': 1, 'prioritized_replay': True, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'final_prioritized_replay_beta': 0.4, 'prioritized_replay_beta_annealing_timesteps': 20000, 'prioritized_replay_eps': 1e-06, 'before_learn_on_batch': None, 'training_intensity': None, 'worker_side_prioritization': False}\n"
     ]
    }
   ],
   "source": [
    "# print out best hyperparameters\n",
    "print(\n",
    "  \"Best hyperparameters found:\",\n",
    "  gs_analysis.best_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the grid search, the best hyper parameter combination is: learning rate 0.001, two hidden layers with 256 hidden neurons and relu activation function. All of the configurations reached the target. It seems that adding more hidden layers did not benefit the network, it only added more time to learn since there are more computations happening. The reason behind this is that the problem is not complex enought meaning it does not require three hidden layers. The configurations with smaller learning rate also required more iterations since the agent learns slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](gsearch_results.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized Configuration\n",
    "Using the configuration that was selected by grid search, a training of 200 iterations was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 10:29:23,607\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2022-04-14 10:29:26,289\tWARNING trainer.py:2347 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-04-14 10:29:26,291\tINFO simple_q.py:154 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "2022-04-14 10:29:26,291\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-04-14 10:29:26,462\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-04-14 10:29:26,463\tWARNING trainer.py:2347 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Trainer.train()`. Instead, you will have to call `Trainer.evaluate()` manually in order to trigger an evaluation run.\n",
      "2022-04-14 10:29:26,511\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.1111111111111111\n",
      "0.1111111111111111\n",
      "0.4\n",
      "0.4\n",
      "0.7272727272727273\n",
      "0.7272727272727273\n",
      "1.3333333333333333\n",
      "1.3333333333333333\n",
      "2.3846153846153846\n",
      "2.3846153846153846\n",
      "3.0714285714285716\n",
      "3.0714285714285716\n",
      "4.0\n",
      "4.0\n",
      "4.875\n",
      "4.875\n",
      "5.705882352941177\n",
      "5.705882352941177\n",
      "6.222222222222222\n",
      "6.222222222222222\n",
      "6.7894736842105265\n",
      "6.7894736842105265\n",
      "7.5\n",
      "7.5\n",
      "7.5\n",
      "8.19047619047619\n",
      "8.19047619047619\n",
      "8.863636363636363\n",
      "8.863636363636363\n",
      "9.521739130434783\n",
      "9.521739130434783\n",
      "9.958333333333334\n",
      "9.958333333333334\n",
      "10.44\n",
      "10.44\n",
      "10.884615384615385\n",
      "10.884615384615385\n",
      "11.185185185185185\n",
      "11.185185185185185\n",
      "11.5\n",
      "11.5\n",
      "12.0\n",
      "12.0\n",
      "12.4\n",
      "12.4\n",
      "12.67741935483871\n",
      "12.67741935483871\n",
      "13.0\n",
      "13.0\n",
      "13.272727272727273\n",
      "13.272727272727273\n",
      "13.617647058823529\n",
      "13.617647058823529\n",
      "13.914285714285715\n",
      "13.914285714285715\n",
      "14.13888888888889\n",
      "14.13888888888889\n",
      "14.324324324324325\n",
      "14.324324324324325\n",
      "14.578947368421053\n",
      "14.578947368421053\n",
      "14.717948717948717\n",
      "14.717948717948717\n",
      "14.975\n",
      "14.975\n",
      "15.097560975609756\n",
      "15.097560975609756\n",
      "15.097560975609756\n",
      "15.214285714285714\n",
      "15.214285714285714\n",
      "15.348837209302326\n",
      "15.348837209302326\n",
      "15.590909090909092\n",
      "15.590909090909092\n",
      "15.755555555555556\n",
      "15.755555555555556\n",
      "15.978260869565217\n",
      "15.978260869565217\n",
      "16.06382978723404\n",
      "16.06382978723404\n",
      "16.229166666666668\n",
      "16.229166666666668\n",
      "16.387755102040817\n",
      "16.387755102040817\n",
      "16.52\n",
      "16.52\n",
      "16.627450980392158\n",
      "16.627450980392158\n",
      "16.76923076923077\n",
      "16.76923076923077\n",
      "16.81132075471698\n",
      "16.81132075471698\n",
      "16.88888888888889\n",
      "16.88888888888889\n",
      "17.0\n",
      "17.0\n",
      "17.071428571428573\n",
      "17.071428571428573\n",
      "17.140350877192983\n",
      "17.140350877192983\n",
      "17.20689655172414\n",
      "17.20689655172414\n",
      "17.322033898305083\n",
      "17.322033898305083\n",
      "17.433333333333334\n",
      "17.433333333333334\n",
      "17.491803278688526\n",
      "17.491803278688526\n",
      "17.532258064516128\n",
      "17.532258064516128\n",
      "17.532258064516128\n",
      "17.634920634920636\n",
      "17.634920634920636\n",
      "17.703125\n",
      "17.703125\n",
      "17.784615384615385\n",
      "17.784615384615385\n",
      "17.818181818181817\n",
      "17.818181818181817\n",
      "17.880597014925375\n",
      "17.880597014925375\n",
      "17.897058823529413\n",
      "17.897058823529413\n",
      "18.0\n",
      "18.0\n",
      "18.085714285714285\n",
      "18.085714285714285\n",
      "18.12676056338028\n",
      "18.12676056338028\n",
      "18.194444444444443\n",
      "18.194444444444443\n",
      "18.21917808219178\n",
      "18.21917808219178\n",
      "18.27027027027027\n",
      "18.27027027027027\n",
      "18.32\n",
      "18.32\n",
      "18.36842105263158\n",
      "18.36842105263158\n",
      "18.454545454545453\n",
      "18.454545454545453\n",
      "18.5\n",
      "18.5\n",
      "18.556962025316455\n",
      "18.556962025316455\n",
      "18.6\n",
      "18.6\n",
      "18.59259259259259\n",
      "18.59259259259259\n",
      "18.609756097560975\n",
      "18.609756097560975\n",
      "18.626506024096386\n",
      "18.626506024096386\n",
      "18.626506024096386\n",
      "18.702380952380953\n",
      "18.702380952380953\n",
      "18.694117647058825\n",
      "18.694117647058825\n",
      "18.74418604651163\n",
      "18.74418604651163\n",
      "18.79310344827586\n",
      "18.79310344827586\n",
      "18.829545454545453\n",
      "18.829545454545453\n",
      "18.876404494382022\n",
      "18.876404494382022\n",
      "18.92222222222222\n",
      "18.92222222222222\n",
      "18.978021978021978\n",
      "18.978021978021978\n",
      "19.02173913043478\n",
      "19.02173913043478\n",
      "19.0752688172043\n",
      "19.0752688172043\n",
      "19.117021276595743\n",
      "19.117021276595743\n",
      "19.157894736842106\n",
      "19.157894736842106\n",
      "19.135416666666668\n",
      "19.135416666666668\n",
      "19.144329896907216\n",
      "19.144329896907216\n"
     ]
    }
   ],
   "source": [
    "# === OPTIMIZED CONFIGURATION ===\n",
    "config_3 = dqn.DEFAULT_CONFIG.copy() # copy the default configuration\n",
    "# === Deep Learning Framework Settings ===\n",
    "config_3[\"framework\"] = \"torch\" # set pytorch for the framework\n",
    "# === Environment Settings ===\n",
    "config_3[\"env\"] = \"Freeway-ramDeterministic-v4\" # load Atari Freeway from OpenAI Gym\n",
    "# === Settings for the Trainer process ===\n",
    "config_3[\"gamma\"] = 0.99 # MDP discount factor\n",
    "config_3[\"lr\"] = 0.001 # the learning rate\n",
    "config_3[\"double_q\"] = True # whether to use double dqn\n",
    "# neural network architecture\n",
    "config_3[\"model\"] = {\n",
    "    \"fcnet_hiddens\": [256,256], # hidden layers and neurons\n",
    "    \"fcnet_activation\": \"relu\" # activation function\n",
    "}\n",
    "# === Exploration Settings ===\n",
    "config_3[\"explore\"] = True\n",
    "config_3[\"exploration_config\"] = {\n",
    "    \"type\": \"EpsilonGreedy\",\n",
    "    # Parameters for the Exploration class' constructor:\n",
    "    \"initial_epsilon\": 1.0,\n",
    "    \"final_epsilon\": 0.01,\n",
    "    \"epsilon_timesteps\": 20000\n",
    "}\n",
    "# === Evaluation Settings ===\n",
    "config_3[\"evaluation_duration\"] = 5\n",
    "config_3[\"evaluation_duration_unit\"] = \"episodes\"\n",
    "config_3[\"evaluation_num_workers\"] = 1\n",
    "config_3[\"evaluation_config\"] = {\n",
    "    \"render_env\": True,\n",
    "    \"explore\": False,\n",
    "}\n",
    "# === Resource Settings ===\n",
    "config_3[\"num_gpus\"] = 0\n",
    "config_3[\"num_cpus_per_worker\"] = 1\n",
    "\n",
    "# based on lab 07 material\n",
    "# initialise ray\n",
    "ray.init()\n",
    "\n",
    "# create our RLlib Trainer.\n",
    "agent_3 = dqn.DQNTrainer(config=config_3)\n",
    "\n",
    "# create list to store rewards\n",
    "avg_rewards_3 = []\n",
    "\n",
    "# begin training\n",
    "for i in range(200):\n",
    "    # Perform one iteration of training the policy with DQN\n",
    "    result = agent_3.train()\n",
    "    #print(pretty_print(result))\n",
    "    print(result['episode_reward_mean'])\n",
    "    avg_rewards_3.append(result['episode_reward_mean'])\n",
    "\n",
    "# shutdown ray\n",
    "#ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Agent 3 Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnJUlEQVR4nO3de5xcdX3/8dd7NxeFIJEkkBASFhSplgLBbTAiujSKENHgpVxEEy9tisqvVcoPQa2grf0hrbcCPyAVAqkQsD8IpooIpSzor4sQQkCuEjCaJSEJ4RIgkJDsp3+cM2EyOTM7ezlz2X0/H495zMz3nJnzydnJfOZ8r4oIzMzMSrXUOwAzM2tMThBmZpbJCcLMzDI5QZiZWSYnCDMzy+QEYWZmmZwgzJqEpHMl/ajC9pWS3lvLmGxoc4KwmpDUKelZSaNreMyQ9OYK24+S9BtJz0naIGmxpMkDON4VkrZIeiG9PSDp/0javb/vWQuSOiT1SHoxvXVL+rGkPy3ZT5L+t6THJL0s6Q+S/lHSqKJ9rkjP+/SisjdL8oCrJuQEYbmT1AYcCQTwofpGs4OHgPdHxFhgb+Ax4OIBvuf5EbEbMAH4NPAO4P9L2nWA75u31RExBtiNJOZHgF9Kmlm0z78A84A56X7HAu8Fril5r2eAf8g9YsudE4TVwhzgTuAKYG7xBknjJP2HpI2S7pb0D5J+VbT9jyTdIukZSY9KOqFo2xWSLpL0s/QX+68lvSnddke6233pr+ITS4OKiLURsbqoaBtQ9oqjLyLilYi4myQhjiNJFkhqkfQ1Sb+XtE7SwsIVRvpLvrvk/JRWG71O0rXpv3eZpEOyjp8e5yxJj6dXRz+WtEcVcUdEdEfE14EfAt9O3+8A4PPAKRHRFRFbI+JB4KPAByS9p+htrgQOLimzJuQEYbUwB7gqvb1f0l5F2y4CXgImkiSP7Qkk/dV9C3A1sCdwMvB/Jf1x0etPBr4BvBFYAXwLICLenW4/JCLGRMS1WYFJmirpOeBl4Azg/AH9S0tExAvpv+HItOhT6e0oYH9gDHBhH95yNvDvwB4k5+UGSSMz9vtr4HjgPSRXR8+SnOu+uB44LP07zAS6I+Ku4h0iYhVJ8j+6qHgT8I+kfwtrXk4QlitJ7wL2BX4cEfcAjwMfT7e1kvwCPSciNkXEQyS/PguOA1ZGxIL0F+sy4DrgY0X7XB8Rd0XEVpIEdGhf4ouIP6RVTOOBr5FUrQy21SRf6ACnAN+NiCci4kXgbOAkSSOqfK97IuL/RcSrwHeB15FUCZX6K+Cr6dXAZuBc4GN9OE4hbgFjSc7PmjL7rSGpUit2KTBV0rF9OJ41GCcIy9tc4OaIeDp9fjWvXSVMAEYAq4r2L368L3B42oj8XPpL/xSSq42Cp4oebyL5Rd5nEfEMSXL6SdaXqKRTihpxf97Ht59MUi8Pya/53xdt+z3JOdir9EVlbD8/EdEDdKfvWWpfYHHReXuYpAqt2uMU4g7gOeBpYFKZ/SYB64sL0qT09+lNfTimNRAnCMuNpNcDJwDvkfSUpKeALwGHpHXn64GtwD5FL5tS9HgVcHtEjC26jYmIz+UU8giSqqw3lG6IiKvSY4+JiKp/FUsaQ9KQ+8u0aDXJl3fBVJJzsJakqm2Xote2svMv8ylF21tIzt1qdrYKOLbk3L0uIp6sNnbgw8CyiHgJ+C9gSnHvpDSGKSRXMLdnvH4BsHv6PtaEnCAsT8eT/Gp9G0nVz6HAW0m+LOdExDaSeu5zJe0i6Y9I2isKfgq8RdInJY1Mb38q6a1VHn8tST1/JkkfkXRg2qA7gaTK5t70amJAJI2W9HbgBpL6/wXppkXAlyTtlyaPfwSuTavIfkvSCP2BtF3ha0Bpt+C3p3GPAL4IbCZpAyh1CfAtSfum8UyQNLuKuCVpsqRzgL8AvgIQEb9N3/MqSe+Q1Jq2BV0H/Dfwn6Xvlf6bzgW+3NtxrTE5QVie5gIL0nr+pwo3kkbZU9IvudNIfmU+BfwbyRfoZtjewHs0cBLJr+SnSHrVVDuW4lzgyrSa5YSM7ZOBm4AXgN8APQz81+6Zkl4gqVJaCNwDvDP9FQ5wOcm/8w7gd8ArwP8CiIjnSXoK/RB4kuSKYodeTcBPgBNJks4ngY+k7RGlfgAsAW5O47kTOLxC3HtLehF4Ebgb+BOgIyJuLtrntDS2H5FU5z1AUkV2fFrdlWUR5dsurMHJCwZZI5H0bWBiRMztdWerK0nfJLlKfHdEPFffaCwPvoKwukrHORycVm1MBz4LLK53XNa7dKzEfLJ7UdkQ4CsIqysl0zksIumJs46ke+R54Q+mWd3lliDS3g0LSbok9gDzI+IH6WjOa4E2YCVwQkQ8m/H6Y0jqUVuBH0bEebkEamZmmfJMEJOASRGxTNJuJI11x5OMIn0mIs6TdBbwxoj4cslrW0l6dLyPpJHubuDkdCCVmZnVQF9GVfZJRKwh7b0QES9Iepik18hsoCPd7Uqgk527wU0HVkTEEwCSrklfVzFBjB8/Ptra2gbnH2BmNgzcc889T0dE6XgbIMcEUUzJbJ7TgF8De6XJg4hYI2nPjJdMZscRtd1U7qIHQFtbG0uXLh14wGZmw4Sk35fblnsvpnQw0HXAFyNiY7UvyyjLrAuTNE/SUklL169fn7WLmZn1Q64JIh0Neh1wVURcnxavTdsnCu0U6zJe2s2OUy6Um06AiJgfEe0R0T5hQuZVkpmZ9UNuCUKSgMuAhyPiu0WblvDaZG1zSUaGlrobOCCdjmAUyUjaJXnFamZmO8vzCuIIkqkA/kzS8vQ2CzgPeJ+kx0h6KZ0HIGlvSTfC9jlcTgN+QTIL5Y/TxUnMzKxG8uzF9CvKT/M7s7QgXdlrVtHzG4Eb84nOzMx646k2zMwsU026uZqZWf90reqic2Un43YZx4ZNGxi3yzjuXXMvANMmTdv+eM4hc5gxZcagHtsJwsysARQngsKX/hte9wa+1/U9tvZsJQiEiOwe/yxYvoDb5t42qEnCCcLMrIaqSQTlVNq2ZdsWOld2OkGYmTW6rlVdLLxvIZBUBW3YtIHnNj9XVSIop9IVxKjWUXS0dQwk5J04QZiZDUBpIrh3zb089eJT/Oyxn/Fqz2uL/VX6cq9EiNaWVk6fcTpjR491G4SZWb1V0ziclQjK6S05FCeCja9s3H6cDZs20NHWMehf/tVwgjCzIafanj/lHvelcbhahfdoxERQjhOEmQ0JhaRQWs8/GF/u/Xn9yJaRfOCADzBxzMTtCaCQsBotEZTjBGFmTaWvvYAGmhygcvtBaSLIs02g1pwgzKwhDaQ7aLFmbBxuFE4QZlZX5XoB/XzFz3l126v00NPn96z2y73S42aqCsqLE4SZ1US13UH7qzQpDPcv98HgBGFmuSkkhbwSQaP3Amp2ThBmNiBZbQWFK4TL7r2s30lhZMtIPjvts676qSMnCDOrSl8bjattHB7KvYCanROEmZVVbmxBNUr3cyJoPrklCEmXA8cB6yLioLTsWuDAdJexwHMRcWjGa1cCLwDbgK0R0Z5XnGa2o4EkhWItamFEywhmvXkWE8dMdCJoQnleQVwBXAgsLBRExImFx5K+Azxf4fVHRcTTuUVnNkxVajPoT/dSNxoPXXmuSX2HpLasbZIEnAD8WV7HN7PXDMZVgRPB8FOvNogjgbUR8ViZ7QHcLCmASyNifrk3kjQPmAcwderUQQ/UrFkNdlLw2ILhp14J4mRgUYXtR0TEakl7ArdIeiQi7sjaMU0e8wHa29sHPumKWRMbjKRQ3L3UVwfDW80ThKQRwEeAt5fbJyJWp/frJC0GpgOZCcJsuCsejFZN+0G5qiL3KrJS9biCeC/wSER0Z22UtCvQEhEvpI+PBr5ZywDNGl1fRyi7qsj6I89urouADmC8pG7gnIi4DDiJkuolSXsDP4yIWcBewOKkHZsRwNURcVNecZo1CycFq7U8ezGdXKb8Uxllq4FZ6eMngEPyisus0Q1kUju3H9hg8khqszorHZfQ1/mLikcou/3ABpMThFkdDHR5TCcFqwUnCLMaqaYLaqXk4KRgteYEYVYDXau6mLlwJpu3bq7YBbV0/iJ3P7V6coIwy1HhquGu1XfxytZXMq8Q3NvIGpUThNkgq6YqyUnBmoEThNkg6q0qSYjZB85m+uTpTgrW8JwgzAZBNVVJLWphdOtozjziTCcGawpOEGb9VM0cSK5KsmbmBGHWD12ruui4soMt27ZkbndVkg0FThBmfVC4ali2Zhmvbsse7eyqJBsqnCDMqlTpqsFzINlQ5ARh1otKVw3T957OYZMO8yA2G5KcIMwqqHTVMLp1NN8/5vtODDZkOUGYVdC5stNXDTZsOUGYZSjuwjqiZcT26bd91WDDiROEWarcim0jW0Zy/IHHexZVG3byXHL0cuA4YF1EHJSWnQv8JbA+3e0rEXFjxmuPAX4AtJIsRXpeXnHa8JS1alu5hXq29mxl+uTpnH3k2bUO06yu8ryCuAK4EFhYUv69iPjnci+S1ApcBLwP6AbulrQkIh7KK1AbHiqt6VxpoZ5RraPoaOuoUZRmjSPPNanvkNTWj5dOB1aka1Mj6RpgNuAEYf3W28jn0uTgxXnM6tMGcZqkOcBS4G8j4tmS7ZOBVUXPu4HDy72ZpHnAPICpU6cOcqg2VGT1RipWulCPk4JZ7RPExcDfA5Hefwf4TMk+ynhd2XUYI2I+MB+gvb2998V8bVgp1xup+ArBo5/NstU0QUTE2sJjSf8K/DRjt25gStHzfYDVOYdmQ4h7I5kNjpomCEmTImJN+vTDwAMZu90NHCBpP+BJ4CTg4zUK0ZpcpbYG90Yy65s8u7kuAjqA8ZK6gXOADkmHklQZrQT+Kt13b5LurLMiYquk04BfkHRzvTwiHswrThtaKrU1uDeSWd/k2Yvp5Iziy8rsuxqYVfT8RmCn8RFm5VTT1uBqJbO+8Uhqa3ql1UpuazAbHE4Q1pQKa0CP22Uc1z103Q7VSm5rMBscThDWdLpWdTFz4Uw2b91MDz07jYJ2W4PZ4HCCsKZRuGq4a/VdvLL1le1JIQhaaKF973ZPw202iJwgrCmUXjUUK6wB7Wm4zQaXE4Q1hc6VnWzZtmWH5CDE7ANnM33ydI+CNsuBE4Q1tEK10nObn6NFLfRET1KllF41nHnEmU4MZjlxgrCGldUY3drSyukzTmfs6LG+ajDLmROENazSaqUgiAjGjh7rLqxmNdDS2w6Szpf0BkkjJd0q6WlJn6hFcDa8dbR1MKp1FC3px7RFLe7CalZD1VxBHB0RZ0r6MMlMq38O3Ab8KNfIbNgqHgQ395C5AJ6S26wOqkkQI9P7WcCiiHhGylqywWzgStsdCo3RHttgVnu9VjEB/yHpEaAduFXSBOCVfMOy4aq03aEnetiybQudKzvrG5jZMNTrFUREnCXp28DGiNgmaRPJGtFmg6ZSd1a3O5jVR9kEIekjGWXFT6/PIyAbftyd1awxVbqC+GB6vyfwTuC/0udHAZ04QdggcXdWs8ZUNkFExKcBJP0UeFthqVBJk4CLahOeDWWuVjJrbNX0YmorWkcaYC3wlt5eJOly4DhgXUQclJb9E8mVyRbgceDTEfFcxmtXAi8A24CtEdFeRZzWRFytZNb4qunF1CnpF5I+JWku8DOScRC9uQI4pqTsFuCgiDgY+C1Qqf7gqIg41MlhaOqtWsnJwaz+qunFdFo6SO7dadH8iFhcxevukNRWUnZz0dM7gY/1IVYbAkrXjo5tsX28g6uVzBpLxQQhqQW4P60i6jUp9NFngGvLbAvgZkkBXBoR8yvEOA+YBzB16tRBDtEGU9ba0fPePs+jpM0aVMUEERE9ku6TNDUi/jBYB5X0VWArcFWZXY6IiNWS9gRukfRIRNxRJsb5wHyA9vb2yNrHGkPnys6d1o6euvtU5r19Xh2jMrNyqmmkngQ8KOku4KVCYUR8qD8HTNsxjgNmRkTmF3pErE7v10laDEwHMhOENY+Otg5Gto7cfgXhKiWzxlZNgvjGYB1M0jHAl4H3RMSmMvvsCrRExAvp46OBbw5WDFZ7hXYHgAuOvYB719wL4PmVzBpcNY3Ut/fnjSUtAjqA8ZK6gXNIei2NJqk2ArgzIk6VtDfww4iYBewFLE63jwCujoib+hOD1V9pu8Po1tHcNvc2JwazJtBrgpD0DuAC4K3AKKAVeCki3lDpdRFxckbxZWX2XU0yWywR8QRwSG9xWXMobXcoTLznBGHW+KoZB3EhcDLwGPB64C/SMrNeFdodCtzuYNY8qlpyNCJWSGqNiG3AAkn/nXNcNkTMmDKDzrmd29sg3O5g1jyqSRCbJI0Clks6H1gD7JpvWNbsihum5xwyh4uPu7jOEZlZX1WTID5JUhV1GvAlYArw0TyDsuZW2jC9YPkCN0ybNaFq2iDeRNLtdGNEfCMiTo+IFXkHZs2rXMO0mTWXahLEp0iql7oknS/pg5LemHNc1sTcMG02NFQzDmIOQDpW4WMka0HsXc1rbXjxgDizoaWacRCfAI4E/gR4mqSL6y9zjsuajAfEmQ091VwFfJ9kcZ9LgNsiYmWeAVlz8oA4s6Gnmiqm8ZL+mGQ9iG9JOgB4NCI+mXt01vBK13d4tSdJEm53MGt+1VQxvQGYCuwLtAG7Q7oMmA1rWes7HH/g8UwcM9HtDmZDQDVVTL8qul0YEd35hmTNImt9h+mTp3P2kZVWkjWzZlFNFdPBkEzDHREv9ba/DR9e38FsaKumimkGySysY4Cpkg4B/ioiPp93cNZ4ulZ10bmyk3G7jGPDpg3uzmo2hFXbi+n9wBKAiLhP0rvzDMoaU9eqLmYunMnmrZvpoYcWtTC6dTS3zrnVicFsCKpmJDURsaqkaFsOsViD61zZyZZtW+hJ+yj0RI+n0TAbwqpJEKskvRMISaMknQE8nHNc1oA62joY1TqKlvRj06IWtzuYDWHVVDGdCvwAmAx0AzcDvbY/SLocOA5YFxEHpWV7ANeSdJddCZwQEc9mvPaY9JitJEuRnldFnJazGVNmcOucW3dog+ho63D1ktkQVU0vpqeBUwrP04n6Pg98q5eXXkEyLcfCorKzgFsj4jxJZ6XPv1z8IkmtJPM9vY8kId0taUlEPNTrv8ZyUdow7aRgNjyUTRCSpgB/RzIx32JgEfBNYE76uKKIuENSW0nxbKAjfXwl0ElJggCmAyvStamRdE36OieIOnDDtNnwVakNYiGwGrgAOAi4k6Sa6eCI+Jt+Hm+viFgDkN7vmbHPZKC4Ubw7LcskaZ6kpZKWrl+/vp9hWTlumDYbvipVMe0REeemj38haS3wpxGxOeeYlFEW5XaOiPnAfID29vay+1n/FBqmi68g3DBtNjxUbINI2xsKX9hPAbtI2hUgIp7px/HWSpoUEWskTQLWZezTTbKsacE+JFcyVgdumDYbvioliN2Be9jxF/2y9D6A/ftxvCXAXOC89P4nGfvcDRwgaT/gSeAk4OP9OJYNgBumzaxsgoiItoG8saRFJA3S4yV1A+eQJIYfS/os8Afgz9N99ybpzjorIrZKOg34BUk318sj4sGBxGJ944ZpM4Mclw2NiJPLbJqZse9qYFbR8xuBG3MKzXpRqWHaCcJs+Khqqg0bXjxi2swgxysIa15umDYzqDJBSHoXcEBELJA0ARgTEb/LNzSrpxlTZjghmA1z1awHcQ7QDhwILABGAj8Cjsg3NKs191wys2LVXEF8GJhG2sU1IlZL2i3XqKzm3HPJzEpV00i9JSKCdDRzYaCcDS2eUsPMSlWTIH4s6VJgrKS/BP4T+Nd8w7Jac88lMytVzXTf/yzpfcBGknaIr0fELblHZjXlnktmVkpJ7dHQ0N7eHkuXLq13GGZmTUPSPRHRnrWtml5ML7DzbKrPA0uBvy2s22BmZkNLNb2Yvksym+rVJBP3nQRMBB4FLue1BYCsCblrq5mVU02COCYiDi96Pl/SnRHxTUlfySswy5+7tppZJdX0YuqRdIKklvR2QtG2odOAMQy5a6uZVVJNgjgF+CTJ4j5r08efkPR64LQcY7OcuWurmVVSTTfXJ4APltn8q8ENx2rJXVvNrJJqejG9Dvgs8MfA6wrlEfGZHOOyGvGkfGZWTjVVTP9G0mvp/cDtJGtEv9DfA0o6UNLyottGSV8s2adD0vNF+3y9v8czM7P+qaYX05sj4s8lzY6IKyVdTbIcaL9ExKPAoQCSWknWnV6csesvI+K4/h7HzMwGppoE8Wp6/5ykg4CngLZBOv5M4PGI+P0gvZ/1omtVFwvvWwjAtEnT3O5gZmVVkyDmS3oj8DVgCTAG+LtBOv5JwKIy22ZIuo9kkN4ZEfHgIB1z2Opa1UXHlR1s2bZle5nHPphZORXbICS1ABsj4tmIuCMi9o+IPSPi0oEeWNIo4EPAv2dsXgbsGxGHABcAN1R4n3mSlkpaun79+oGGNaR1ruzk1W2v7lDmsQ9mVk7FBBERPeQ31uFYYFlErM047saIeDF9fCMwUtL4MjHOj4j2iGifMGFCTqEODR1tHYxsHblDmcc+mFk51VQx3SLpDOBa4KVCYUQ8M8Bjn0yZ6iVJE4G1ERGSppMksg0DPN6wN2PKDDrndroNwsyq0ut035J+l1EcEbF/vw8q7QKsAvaPiOfTslPTN75E0mnA54CtwMvA6RHx3729r6f7NjPrmwFN9x0R+w12QBGxCRhXUnZJ0eMLgQsH+7hmZla9akZS7wKcDkyNiHmSDgAOjIif5h6dDZin8zaz/qqmDWIBcA/wzvR5N0nPIyeIBufpvM1sIKqZauNNEXE+6YC5iHiZZOEga3CeztvMBqKaBLElndo7ACS9Cdica1Q2KDydt5kNRDVVTOcCNwFTJF0FHAF8KseYbJB4Om8zG4heu7kCSBoHvIOkaunOiHg678D6w91czcz6ZkDdXCUtIRnQtiQiXuptf6s/91wys8FQTRXTd4ATgfMk3UUyovqnEfFKrpFZv7jnkpkNll4bqSPi9oj4PLA/MB84gWR9amtA7rlkZoOlmisI0l5MHyS5kjgMuDLPoKz/Cj2Xiq8g3HPJzPqjmjaIa4HDSXoyXQR0prO8WgNyzyUzGyzVjqT+eERsA5B0hKSPR8QX8g3N+sIN02Y22KqZrO8mSYdKOpmkiul3wPW5R2ZVc8O0meWhbCO1pLdI+rqkh0lmVu0mGTdxVERcULMIrVdumDazPFS6gngE+CXwwYhYASDpSzWJyvrEDdNmlodKCeKjwEnAbZJuAq7Bk/Q1JDdMm1keqllRblfgeJIlQv+MpIvr4oi4Offo+shTbZiZ9U2lqTaqGSj3UkRcFRHHAfsAy4GzBhjQSkm/kbRc0k7f6Er8i6QVku6XdNhAjmdmZn1X1UC5goh4Brg0vQ3UURUm/TsWOCC9HQ5cnN6bmVmNVLMeRD3MBhZG4k5grKRJ9Q7KzGw46dMVxCAK4GZJAVwaEfNLtk8GVhU9707L1pS+kaR5wDyAqVOn5hNtg/LgODPLU70SxBERsVrSnsAtkh6JiDuKtmf1lspsTU+Ty3xIGqkHP9TG5MFxZpa3ulQxRcTq9H4dsBiYXrJLNzCl6Pk+wOraRNccPDjOzPJW8wQhaVdJuxUeA0cDD5TstgSYk/ZmegfwfETsVL00nHm9aTPLWz2qmPYCFksqHP/qdL6nUwEi4hLgRmAWsALYBHy6DnE2NA+OM7O8VbUmdbPwQDkzs74Z0EA5MzMbnpwgzMwskxOEmZllqtc4COsnD44zs1pxgmgiHhxnZrXkKqYm4sFxZlZLThBNxIPjzKyWXMXURDw4zsxqyQmiycyYMsMJwcxqwlVMZmaWyQnCzMwyOUGYmVkmt0E0AQ+OM7N6cIJocB4cZ2b14iqmBufBcWZWL04QDc6D48ysXlzF1OA8OM7M6qXmCULSFGAhMBHoAeZHxA9K9ukAfgL8Li26PiK+WcMwG4oHx5lZPdTjCmIr8LcRsUzSbsA9km6JiIdK9vtlRBxXh/jMzIw6JIiIWAOsSR+/IOlhYDJQmiCGta5VXSy8byEAcw6Z4ysIM6u5urZBSGoDpgG/ztg8Q9J9wGrgjIh4sMx7zAPmAUydOjWnSGura1UXHVd2sGXbFgAWLF/AbXNvc5Iws5qqWy8mSWOA64AvRsTGks3LgH0j4hDgAuCGcu8TEfMjoj0i2idMmJBbvLXUubKTV7e9uv25u7WaWT3UJUFIGkmSHK6KiOtLt0fExoh4MX18IzBS0vgah1k3HW0djGwduf25u7WaWT3UoxeTgMuAhyPiu2X2mQisjYiQNJ0kkW2oYZh1NWPKDDrndroNwszqqh5tEEcAnwR+I2l5WvYVYCpARFwCfAz4nKStwMvASRERdYi1bty11czqrR69mH4FqJd9LgQurE1EZmaWxVNtmJlZJicIMzPL5ARhZmaZnCDMzCyTZ3NtIJ5ew8waiRNEg/D0GmbWaFzF1CA8vYaZNRoniAbh6TXMrNG4iqlBeHoNM2s0ThANxNNrmFkjcYKoM/dcMrNG5QRRR+65ZGaNzI3UdeSeS2bWyJwg6sg9l8yskbmKqQ6K2x0uOPYC7l1zL+A2CDNrLE4QNVba7jC6dbTbHcysIbmKqcbc7mBmzaIuCULSMZIelbRC0lkZ2yXpX9Lt90s6rB5xDqauVV187qef464n72JEy2sXbm53MLNGVfMqJkmtwEXA+4Bu4G5JSyLioaLdjgUOSG+HAxen97kobhOYNmna9jaBaZOmsWHTBsbtMm6Hsv48vuzey3i1J7lyGNkykuMPPJ6JYya63cHMGlY92iCmAysi4gkASdcAs4HiBDEbWBgRAdwpaaykSRGxZrCDKW0TKCVEEAM6Rul7bO3ZyvTJ0zn7yLMH9L5mZnmqRxXTZGBV0fPutKyv+wAgaZ6kpZKWrl+/vs/BlLYJlBpocsh6D1crmVkzqMcVhDLKSr+Fq9knKYyYD8wHaG9v7/O3eWEsQp5XEC1qYUTLCGa9eZarlcysadQjQXQDU4qe7wOs7sc+g6J0FtU82iA2bNpAR1uHk4KZNZV6JIi7gQMk7Qc8CZwEfLxknyXAaWn7xOHA83m0PxR4FlUzs53VPEFExFZJpwG/AFqByyPiQUmnptsvAW4EZgErgE3Ap2sdp5nZcFeXkdQRcSNJEiguu6TocQBfqHVcZmb2Go+kNjOzTE4QZmaWyQnCzMwyOUGYmVkmJe3BQ4Ok9cDv+/CS8cDTOYUzEI0aFzi2/mjUuKBxY2vUuGDoxbZvREzI2jCkEkRfSVoaEe31jqNUo8YFjq0/GjUuaNzYGjUuGF6xuYrJzMwyOUGYmVmm4Z4g5tc7gDIaNS5wbP3RqHFB48bWqHHBMIptWLdBmJlZecP9CsLMzMpwgjAzs0zDMkFIOkbSo5JWSDqrzrFMkXSbpIclPSjpb9LycyU9KWl5eptVh9hWSvpNevyladkekm6R9Fh6/8Y6xHVg0XlZLmmjpC/W65xJulzSOkkPFJWVPU+Szk4/e49Ken+N4/onSY9Iul/SYklj0/I2SS8XnbtLyr5xfrGV/fvV6pxViO3aorhWSlqeltfsvFX4rsjvsxYRw+pGMsX448D+wCjgPuBtdYxnEnBY+ng34LfA24BzgTPqfK5WAuNLys4HzkofnwV8uwH+nk8B+9brnAHvBg4DHujtPKV/2/uA0cB+6WextYZxHQ2MSB9/uyiutuL96nTOMv9+tTxn5WIr2f4d4Ou1Pm8Vvity+6wNxyuI6cCKiHgiIrYA1wCz6xVMRKyJiGXp4xeAhymz/naDmA1cmT6+Eji+fqEAMBN4PCL6MoJ+UEXEHcAzJcXlztNs4JqI2BwRvyNZ82R6reKKiJsjYmv69E6S1Rprrsw5K6dm56y32CQJOAFYlNfxy6nwXZHbZ204JojJwKqi5900yBeypDZgGvDrtOi0tCrg8npU5ZCsA36zpHskzUvL9op0db/0fs86xFXsJHb8z1rvc1ZQ7jw10ufvM8DPi57vJ+leSbdLOrJOMWX9/RrpnB0JrI2Ix4rKan7eSr4rcvusDccEoYyyuvf1lTQGuA74YkRsBC4G3gQcCqwhuayttSMi4jDgWOALkt5dhxjKkjQK+BDw72lRI5yz3jTE50/SV4GtwFVp0RpgakRMA04Hrpb0hhqHVe7v1xDnLHUyO/4gqfl5y/iuKLtrRlmfzttwTBDdwJSi5/sAq+sUCwCSRpL8wa+KiOsBImJtRGyLiB7gX8nxkrqciFid3q8DFqcxrJU0KY17ErCu1nEVORZYFhFroTHOWZFy56nunz9Jc4HjgFMiraxOqyE2pI/vIamvfkst46rw96v7OQOQNAL4CHBtoazW5y3ru4IcP2vDMUHcDRwgab/0F+hJwJJ6BZPWaV4GPBwR3y0qn1S024eBB0pfm3Ncu0rarfCYpHHzAZJzNTfdbS7wk1rGVWKHX3P1Pmclyp2nJcBJkkZL2g84ALirVkFJOgb4MvChiNhUVD5BUmv6eP80ridqFVd63HJ/v7qesyLvBR6JiO5CQS3PW7nvCvL8rNWi9b3RbsAskh4AjwNfrXMs7yK57LsfWJ7eZgH/BvwmLV8CTKpxXPuT9IC4D3iwcJ6AccCtwGPp/R51Om+7ABuA3YvK6nLOSJLUGuBVkl9tn610noCvpp+9R4FjaxzXCpJ66cJn7ZJ034+mf+f7gGXAB+twzsr+/Wp1zsrFlpZfAZxasm/NzluF74rcPmueasPMzDINxyomMzOrghOEmZllcoIwM7NMThBmZpbJCcLMzDI5QZiVkLRNO84WW3HGX0mnSpozCMddKWn8QN/HbLC4m6tZCUkvRsSYOhx3JdAeEU/X+thmWXwFYVal9Bf+tyXdld7enJafK+mM9PFfS3oonXDumrRsD0k3pGV3Sjo4LR8n6eZ0ordLKZo7R9In0mMsl3SppNb0doWkB5Ss0/GlOpwGG0acIMx29vqSKqYTi7ZtjIjpwIXA9zNeexYwLSIOBk5Ny74B3JuWfQVYmJafA/wqkonelgBTASS9FTiRZLLEQ4FtwCkkk9hNjoiDIuJPgAWD9Q82yzKi3gGYNaCX0y/mLIuK7r+Xsf1+4CpJNwA3pGXvIpmSgYj4r/TKYXeShWk+kpb/TNKz6f4zgbcDdyfT7/B6kgnY/gPYX9IFwM+Am/v57zOriq8gzPomyjwu+ABwEckX/D3pDKCVpl3Oeg8BV0bEoentwIg4NyKeBQ4BOoEvAD/s57/BrCpOEGZ9c2LRfVfxBkktwJSIuA04ExgLjAHuIKkiQlIH8HQk8/gXlx8LFBbIuRX4mKQ90217SNo37eHUEhHXAX9HsiymWW5cxWS2s9crXZQ+dVNEFLq6jpb0a5IfVyeXvK4V+FFafSTgexHxnKRzgQWS7gc28drUzN8AFklaBtwO/AEgIh6S9DWS1fxaSGYV/QLwcvo+hR92Zw/av9gsg7u5mlXJ3VBtuHEVk5mZZfIVhJmZZfIVhJmZZXKCMDOzTE4QZmaWyQnCzMwyOUGYmVmm/wGHlS2Q0KNB3gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the rewards of agent 3\n",
    "plt.title(\"Agent 3 - Double DQN\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Rewards\")\n",
    "a3 = plt.plot(avg_rewards_3, 'g.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Agent 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': 25.0,\n",
       "  'episode_reward_min': 24.0,\n",
       "  'episode_reward_mean': 24.2,\n",
       "  'episode_len_mean': 2048.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 5,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [24.0, 24.0, 24.0, 25.0, 24.0],\n",
       "   'episode_lengths': [2048, 2048, 2048, 2048, 2048]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.08993371833687287,\n",
       "   'mean_inference_ms': 0.558637476167622,\n",
       "   'mean_action_processing_ms': 0.025392661231090963,\n",
       "   'mean_env_wait_ms': 0.6782928489518927,\n",
       "   'mean_env_render_ms': 1.1537799075320065},\n",
       "  'off_policy_estimator': {},\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate agent for 5 episodes\n",
    "agent_3.evaluate()\n",
    "# shutdown ray\n",
    "ray.shutdown()\n",
    "#show agent_2 evaluation metrics\n",
    "agent_3.evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 3 with optimal configuration reached an average of 20 rewards after 200 iterations and from that point it started reaching plateau. During evaluation it was able to achieve an average of 24.2 rewards, after playing 5 episodes, which is pretty impresive considering back in the day Activition used to send a cloth \"Save The Chicken Foundation\" patch to every player that managed to score 20 or more points on either Road 3 or Road 7 and sent in a photograph of thei television screen. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- [1] Freeway (video game), Wikipedia https://en.wikipedia.org/wiki/Freeway_(video_game)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9285b2cc289002d68e5229deff99f56373611dfe6f7a22153195c44f04bc854f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
